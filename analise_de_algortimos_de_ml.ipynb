{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "a parte de limpesa de dados ela e relacionada e variada ou seja ela varia bastante"
      ],
      "metadata": {
        "id": "l7ORPhWus1bI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "libs"
      ],
      "metadata": {
        "id": "eKfXbazss9m5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1grVH9AhsqbR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from dataclasses import dataclass\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "classe de leitura do dataset, estamos usando a entrada de arquivo do google colab mas pode ser editado para um caminho do seu sistema"
      ],
      "metadata": {
        "id": "xTiNm_V0tEY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_my_dataset(csv_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Lê um arquivo CSV contendo colunas:\n",
        "    partida_id, rodata, clube, cartao, atleta, num_camisa, posicao, minuto.\n",
        "\n",
        "    Retorna um DataFrame do pandas com essas colunas.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(csv_path)\n",
        "    print(\"Colunas encontradas no dataset:\", data.columns)\n",
        "\n",
        "    # Exemplo de acesso às colunas\n",
        "    print(\"\\nPrimeiras linhas do dataset:\")\n",
        "    print(data.head())\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "mzvj73pPtA20"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "desing patterns - factory com a implementação de 7 algoritimos, podendo gerar facil manutenção para adicionar ou remover"
      ],
      "metadata": {
        "id": "PvQq61ootU1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models():\n",
        "    \"\"\"\n",
        "    Retorna uma lista de modelos com configurações padrão.\n",
        "    \"\"\"\n",
        "    models = []\n",
        "    models.append(Model(\"Regressão Logística\",\n",
        "                        LogisticRegression(max_iter=1000, random_state=42)))\n",
        "    models.append(Model(\"Árvore de Decisão\",\n",
        "                        DecisionTreeClassifier(max_depth=5, random_state=42)))\n",
        "    models.append(Model(\"KNN\",\n",
        "                        KNeighborsClassifier(n_neighbors=5)))\n",
        "    models.append(Model(\"Naive Bayes (Gaussian)\",\n",
        "                        GaussianNB()))\n",
        "    models.append(Model(\"SVM\",\n",
        "                        SVC(probability=True, random_state=42)))\n",
        "    models.append(Model(\"Rede Neural (MLP)\",\n",
        "                        MLPClassifier(hidden_layer_sizes=(100, 50),\n",
        "                                      max_iter=1000,\n",
        "                                      random_state=42)))\n",
        "    models.append(Model(\"Random Forest\",\n",
        "                        RandomForestClassifier(n_estimators=100, random_state=42)))\n",
        "    models.append(Model(\"Gradient Boosting\",\n",
        "                        GradientBoostingClassifier(n_estimators=100, random_state=42)))\n",
        "    if xgboost_available:\n",
        "        models.append(Model(\"XGBoost\",\n",
        "                            XGBClassifier(n_estimators=100, random_state=42)))\n",
        "    return models"
      ],
      "metadata": {
        "id": "OWa5npVltTrQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot de matriz de confusao"
      ],
      "metadata": {
        "id": "ibFSQ5ODwIGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, title=\"Matriz de Confusão\"):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Previsto\")\n",
        "    plt.ylabel(\"Verdadeiro\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "1vtu0ICdwGEU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "metricas de avaliação ROC\n",
        "\n",
        "conhecido como Curva de característica de operação do receptor"
      ],
      "metadata": {
        "id": "UjVKoQHswNAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. True Positive Rate (TPR / Sensibilidade)\n",
        "A **taxa de verdadeiros positivos** indica o quão bem o modelo detecta os positivos reais.\n",
        "Fórmula:\n",
        "$$\n",
        "\\text{TPR} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "Onde:  \n",
        "- **TP** = True Positives (Verdadeiros Positivos)  \n",
        "- **FN** = False Negatives (Falsos Negativos)\n",
        "---\n",
        "### 2. False Positive Rate (FPR)\n",
        "A **taxa de falsos positivos** mostra com que frequência o modelo classifica incorretamente os negativos como positivos.\n",
        "Fórmula:\n",
        "$$\n",
        "\\text{FPR} = \\frac{FP}{FP + TN}\n",
        "$$\n",
        "Onde:  \n",
        "- **FP** = False Positives (Falsos Positivos)  \n",
        "- **TN** = True Negatives (Verdadeiros Negativos)"
      ],
      "metadata": {
        "id": "p15OI_n5zTA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relação com a Área Sob a Curva (AUC-ROC)**\n",
        "\n",
        "A **AUC-ROC** (Área Sob a Curva ROC) mede a capacidade do modelo de distinguir entre classes. Quanto maior a AUC (próxima de 1), melhor o modelo. A fórmula da AUC é calculada integrando a curva ROC:\n",
        "\n",
        "$$\n",
        "\\text{AUC} = \\int_{0}^{1} \\text{TPR}(\\text{FPR}) \\, d(\\text{FPR})\n",
        "$$"
      ],
      "metadata": {
        "id": "mBzLgYx02GUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#analisar a area do grafico do plot\n",
        "def plot_roc_curve(y_true, y_scores, title=\"Curva ROC\"):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"AUC = {roc_auc:.2f}\")\n",
        "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "    plt.xlabel(\"Falso Positivo\")\n",
        "    plt.ylabel(\"Verdadeiro Positivo\")\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SujpcjXiwMQr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparação do piperline\n",
        "\n",
        "filtrando a parte de valores com NULL e NAN e removendo duplicatas"
      ],
      "metadata": {
        "id": "73qq226-zj4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#talvez seja melhor usar uma lib para a analise exploratoria de dados\n",
        "\n",
        "def pipeline_cartoes():\n",
        "    \"\"\"\n",
        "    Pipeline para análise e predição de cartões em partidas de futebol\n",
        "    usando o dataset com colunas: partida_id, rodata, clube, cartao, atleta,\n",
        "    num_camisa, posicao, minuto.\n",
        "    \"\"\"\n",
        "    # ----------------------------------------------------------------\n",
        "    # Passo 1: Carregar os dados\n",
        "    # ----------------------------------------------------------------\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        dataset_path = list(uploaded.keys())[0]\n",
        "        print(f\"Arquivo carregado: {dataset_path}\")\n",
        "    except:\n",
        "        print(\"Erro ao carregar arquivo. Usando caminho fixo.\")\n",
        "        dataset_path = \"seu_dataset.csv\"  # Substitua pelo caminho correto\n",
        "\n",
        "    data = read_my_dataset(dataset_path)\n",
        "\n",
        "    # ----------------------------------------------------------------\n",
        "    # Passo 2: Limpeza e Exploração de Dados\n",
        "    # ----------------------------------------------------------------\n",
        "    print(\"\\n=== Explorando dados de cartões ===\")\n",
        "\n",
        "    # Verificar valores nulos\n",
        "    print(\"\\nValores nulos por coluna:\")\n",
        "    print(data.isnull().sum())\n",
        "\n",
        "    # Remover duplicatas\n",
        "    data_cleaned = data.drop_duplicates()\n",
        "    print(f\"\\nLinhas após remover duplicatas: {data_cleaned.shape[0]} (original: {data.shape[0]})\")\n",
        "\n",
        "    # Preencher valores nulos se necessário\n",
        "    for col in data_cleaned.columns:\n",
        "        if data_cleaned[col].isnull().sum() > 0:\n",
        "            if data_cleaned[col].dtype in ['float64', 'int64']:\n",
        "                data_cleaned[col].fillna(data_cleaned[col].mean(), inplace=True)\n",
        "            else:\n",
        "                data_cleaned[col].fillna(data_cleaned[col].mode()[0], inplace=True)\n",
        "\n",
        "    # Análise básica dos cartões\n",
        "    if 'cartao' in data_cleaned.columns:\n",
        "        print(\"\\nDistribuição de cartões:\")\n",
        "        cartoes_count = data_cleaned['cartao'].value_counts()\n",
        "        print(cartoes_count)\n",
        "\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        sns.countplot(x='cartao', data=data_cleaned)\n",
        "        plt.title('Distribuição de Cartões')\n",
        "        plt.xlabel('Tipo de Cartão')\n",
        "        plt.ylabel('Quantidade')\n",
        "        plt.show()\n",
        "\n",
        "    # Análise por posição (se disponível)\n",
        "    if 'posicao' in data_cleaned.columns:\n",
        "        print(\"\\nCartões por posição:\")\n",
        "        cartoes_posicao = pd.crosstab(data_cleaned['posicao'], data_cleaned['cartao'])\n",
        "        print(cartoes_posicao)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        cartoes_posicao.plot(kind='bar', stacked=True)\n",
        "        plt.title('Cartões por Posição')\n",
        "        plt.xlabel('Posição')\n",
        "        plt.ylabel('Quantidade')\n",
        "        plt.legend(title='Tipo de Cartão')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "         # Codificar variáveis categóricas\n",
        "    data_encoded = data_cleaned.copy()\n",
        "    categorical_columns = data_cleaned.select_dtypes(include=['object']).columns\n",
        "\n",
        "    label_encoders = {}\n",
        "    for col in categorical_columns:\n",
        "        le = LabelEncoder()\n",
        "        data_encoded[col] = le.fit_transform(data_cleaned[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "        print(f\"Codificação para {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
        "\n",
        "    # Definir variável alvo (neste caso vamos prever o tipo de cartão)\n",
        "    if 'cartao' in data_encoded.columns:\n",
        "        target = 'cartao'\n",
        "        print(f\"\\nPrevisão da variável alvo: {target}\")\n",
        "        X = data_encoded.drop(columns=[target])\n",
        "        y = data_encoded[target]\n",
        "\n",
        "        # Dividir em treino e teste\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.3, stratify=y, random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"Tamanho do conjunto de treino: {X_train.shape[0]} amostras\")\n",
        "        print(f\"Tamanho do conjunto de teste: {X_test.shape[0]} amostras\")\n",
        "\n",
        "        # Normalizar as features numéricas\n",
        "        numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "        if len(numeric_cols) > 0:\n",
        "            scaler = MinMaxScaler()\n",
        "            X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
        "            X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])"
      ],
      "metadata": {
        "id": "G41sdYB2zvVf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparacao das melhores features com o kbeast"
      ],
      "metadata": {
        "id": "kIPwFww70htR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selector = SelectKBest(score_func=f_classif, k='all')\n",
        "        selector.fit(X_train, y_train)\n",
        "\n",
        "        # Obter scores de importância\n",
        "        feature_scores = pd.DataFrame({\n",
        "            'Feature': X.columns,\n",
        "            'Score': selector.scores_\n",
        "        })\n",
        "        feature_scores = feature_scores.sort_values('Score', ascending=False)\n",
        "        print(\"\\nImportância das features:\")\n",
        "        print(feature_scores.head(10))  # Top 10 features\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='Score', y='Feature', data=feature_scores.head(10))\n",
        "        plt.title('Top 10 Features Mais Importantes')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Selecionar as top features (opcional)\n",
        "        k_best = min(10, len(X.columns))  # Limitar a 10 ou menos\n",
        "        top_features = feature_scores['Feature'].head(k_best).values\n",
        "        X_train_selected = X_train[top_features]\n",
        "        X_test_selected = X_test[top_features]\n",
        "\n",
        "        print(f\"\\nUsando as top {k_best} features para modelagem\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "JOF_c6780lxk",
        "outputId": "9fa5f600-b2ce-4948-fd35-07034b9234fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-17-5c5f4c6c3280>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-5c5f4c6c3280>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    selector.fit(X_train, y_train)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "treinamento e avaliacao dos modelos"
      ],
      "metadata": {
        "id": "6Co_1Nwj0x2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " print(\"\\n=== Treinamento e avaliação de modelos ===\")\n",
        "        models = get_models()\n",
        "        results = []\n",
        "\n",
        "        for model in models:\n",
        "            print(f\"\\nTreinando {model.nome}...\")\n",
        "            clf = model.estimator\n",
        "            clf.fit(X_train_selected, y_train)\n",
        "            y_pred = clf.predict(X_test_selected)\n",
        "\n",
        "        #usando o precision e recall pelo metivo do banco de dado\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "            precision = precision_score(y_test, y_pred, average='weighted')\n",
        "            recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "            results.append({\n",
        "                'Modelo': model.nome,\n",
        "                'Acurácia': acc,\n",
        "                'F1 Score': f1,\n",
        "                'Precisão': precision,\n",
        "                'Recall': recall\n",
        "            })\n",
        "\n",
        "            print(f\"Acurácia: {acc:.4f}\")\n",
        "            print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "            # Matriz de confusão\n",
        "            plot_confusion_matrix(y_test, y_pred, title=f\"Matriz de Confusão - {model.nome}\")\n",
        "\n",
        "            # Importância das features (para modelos que suportam)\n",
        "            if hasattr(clf, 'feature_importances_'):\n",
        "                importances = pd.DataFrame({\n",
        "                    'Feature': top_features,\n",
        "                    'Importance': clf.feature_importances_\n",
        "                }).sort_values('Importance', ascending=False)\n",
        "\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                sns.barplot(x='Importance', y='Feature', data=importances)\n",
        "                plt.title(f'Importância das Features - {model.nome}')\n",
        "                plt.tight_layout()\n",
        "                plt.show()"
      ],
      "metadata": {
        "id": "VcMbzuwd0rrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparacao dos modelos"
      ],
      "metadata": {
        "id": "fSub0mkG2sCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " results_df = pd.DataFrame(results)\n",
        "        print(\"\\n=== Comparação dos Modelos ===\")\n",
        "        print(results_df)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        results_df_melted = pd.melt(results_df,\n",
        "                                   id_vars=['Modelo'],\n",
        "                                   value_vars=['Acurácia', 'F1 Score', 'Precisão', 'Recall'],\n",
        "                                   var_name='Métrica', value_name='Valor')\n",
        "        sns.barplot(x='Modelo', y='Valor', hue='Métrica', data=results_df_melted)\n",
        "        plt.title('Comparação de Modelos')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.ylim(0, 1)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"A coluna 'cartao' não foi encontrada no dataset.\")"
      ],
      "metadata": {
        "id": "CqFnjA2r2rGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "plots dos algoritimos"
      ],
      "metadata": {
        "id": "jihzNJCB26tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "   # Plot da Curva ROC (para binário)\n",
        "        if hasattr(clf, \"predict_proba\"):\n",
        "            y_prob = clf.predict_proba(X_test)[:, 1]\n",
        "            plot_roc_curve(y_test, y_prob, title=f\"{model.nome} - Curva ROC\")\n",
        "\n",
        "        # Exemplo de visualização da árvore\n",
        "        if isinstance(clf, DecisionTreeClassifier):\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plot_tree(clf, filled=True, feature_names=[f\"feat_{i}\" for i in range(X.shape[1])])\n",
        "            plt.title(f\"{model.nome} - Árvore de Decisão\")\n",
        "            plt.show()\n",
        "\n",
        "        # Importância de features (RandomForest, GradientBoosting, XGBoost)\n",
        "        if isinstance(clf, (RandomForestClassifier, GradientBoostingClassifier)):\n",
        "            importances = clf.feature_importances_\n",
        "            indices = np.argsort(importances)[::-1]\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            plt.bar(range(X.shape[1]), importances[indices], color=\"r\", align=\"center\")\n",
        "            plt.xticks(range(X.shape[1]), [f\"feat_{i}\" for i in indices], rotation=45)\n",
        "            plt.title(f\"{model.nome} - Importância das Features\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        if xgboost_available and isinstance(clf, XGBClassifier):\n",
        "            importances = clf.feature_importances_\n",
        "            indices = np.argsort(importances)[::-1]\n",
        "            plt.figure(figsize=(6, 4))\n",
        "            plt.bar(range(X.shape[1]), importances[indices], color=\"g\", align=\"center\")\n",
        "            plt.xticks(range(X.shape[1]), [f\"feat_{i}\" for i in indices], rotation=45)\n",
        "            plt.title(f\"{model.nome} - Importância das Features (XGBoost)\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    print(f\"\\nMelhor modelo no dataset aleatório: {melhor_modelo} com acurácia de {melhor_acc:.4f}\")"
      ],
      "metadata": {
        "id": "8wL_R2Mx4Afe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=== Pipeline para Análise de Cartões em Partidas de Futebol ===\")\n",
        "    print(\"Carregue seu dataset com colunas: partida_id, rodata, clube, cartao, atleta, num_camisa, posicao, minuto\")\n",
        "    pipeline_cartoes()\n"
      ],
      "metadata": {
        "id": "gKlkOi814II9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}